# -*- coding: utf-8 -*-
"""ML과제 Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ry5W9giarmNw_uOP4zcyOCb7dn0gMChv

#Topic
# Unsupervised Learning practice
# Classification Problem with Mall data
"""

# Reading the data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('/content/Mall_Customers.csv')
df.head()

# Age
df['Age'].describe()

df.info()

# find the missing value
df.isnull().sum()

"""EDA 데이터를 알아보자"""

# drop CustomerID
df = df.drop('CustomerID', axis=1)

# Encode Male and Female
df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})

## scatterplot Gender vs Spending Score
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Gender', y='Spending Score (1-100)')
plt.title('Gender vs Spending Score')
plt

## scatterplot Age vs Spending Score
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Age', y='Spending Score (1-100)')
plt.title('Age vs Spending Score')
plt.show()
## scatter plot Annual Income vs spending score
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)')
plt.title('Annual Income vs Spending Score')
plt.show()

"""비선형적으로 보임 그래도 모르니 ,,,"""

# correlate Each feature with spending Score
correlation_matrix = df.corr()
correlation_matrix

# Look up the range of each feature
df.describe()

"""# Classication
데이터들의 상관성 분석

"""

## Create new features with Age(10- 19, 20 - 29, 30- 39, 40- 49, 50- 59, 60- 69, 70 +)
age_bins = [10, 20, 30, 40, 50, 60, 70, 80]

## split by age_bins
# Changed 'age' to 'age_bins' in the bins argument
df['Age_Group'] = pd.cut(df['Age'], bins=age_bins, labels=['10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70+'])
df['Age_Group'] = df['Age_Group'].astype(str)
df.head()
# view as histogram
df['Age_Group'].value_counts()


# display
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age_Group')
plt.title('Age Group Distribution')
plt.show()

## Create one Feature for Age Group 10,20,30,40,50,60,70 instead of 10- 19, 20 - 29, 30- 39, 40- 49, 50- 59, 60- 69, 70 +
df['Age_Group2'] = df['Age_Group'].replace(['10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70+'], ['10', '20', '30', '40', '50', '60', '70'])
## make Age_Group2 as int
df['Age_Group2'] = df['Age_Group2'].astype(int)
df.head()

df.info()

## legend with Spending score
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age_Group', hue='Spending Score (1-100)')
plt.title('Age Group Distribution with Spending Score')
plt
## legend with Gender
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age_Group', hue='Gender')
plt.title('Age Group Distribution with Gender')

df.describe()

## Annual Income Group by 15- 30, 30- 50, 50-100, 100+
income_bins = [10, 30, 50, 100, 150]
df['Income_Group'] = pd.cut(df['Annual Income (k$)'], bins=income_bins, labels=['10-30', '30-50', '50-100', '100+'])
df

## Make spending score 10,20,30,40,50,60,70,80,90,100 and create new column name Spend
df['Spend'] = pd.cut(df['Spending Score (1-100)'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100'])
df

## Create histogram between Annual income group and spenaind score
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Income_Group', hue='Spend', multiple='stack')
plt.title('Annual Income Group Distribution with Spending Score')
plt
# legend with Gender
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Income_Group', hue='Gender')
plt.title('Annual Income Group Distribution with Gender')

## Create histogram between AgeGroup2 and spend
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Age_Group2', hue='Spend', multiple='stack')
plt.title('Age Group Distribution with Spending Score')
plt
# legend by Gender
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age_Group2', hue='Gender')
plt.title('Age Group Distribution with Gender')

"""## 아무래도 clustering 을 하기에 정규화 대신 표준화를 택함"""

from sklearn.preprocessing import StandardScaler

# Select relevant numerical columns for clustering
numerical_columns = ["Age", "Annual Income (k$)", "Spending Score (1-100)", "Gender"]
data_numerical = df[numerical_columns]

# Normalize the data using StandardScaler
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data_numerical)

# Convert the normalized data back to a DataFrame for better visualization
data_normalized_df = pd.DataFrame(data_normalized, columns=numerical_columns)

# Display the first few rows of the normalized dataset
data_normalized_df.head()

pd.DataFrame(data_normalized).describe()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 데이터 준비 (이미 표준화된 data_normalized 사용)
numerical_columns = ["Age", "Annual Income (k$)", "Spending Score (1-100)"]

# PCA 수행
pca = PCA(n_components=3)  # 최대 3개의 주성분 생성
pca_result = pca.fit_transform(data_normalized)

# 주성분별 설명 분산 비율 출력
explained_variance = pca.explained_variance_ratio_
print("Explained variance ratio by each component:", explained_variance)
print("Cumulative explained variance:", explained_variance.cumsum())
##0.44266167 0.33308378 0.2242545  않좋음... 비선형으로 추측 ... 그래도 일단 ㄱㄱ

"""## PC1 + PC2 => 59,92%
## PC1 + PC2 + PC3 => 83.18%

EVR 이 아무래도 0.9 이상이 아니다보니, 3개의 주성분으로는 데이터의 다른 방식으로 변수를 추가적으로 만들필요가 있음.
추가적으로 주성분이 필요하거나, 데이터 전처리에서 다른 방식으로 시도를 해봐야하나 고려해야하지만 그냥 일단 시작.
"""

# 설명 분산 비율 누적 그래프
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--')
plt.title("Cumulative Explained Variance by PCA Components")
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.grid(True)
plt.show()

"""다른 방식으로 EVR 을 보여줌. 보시다 싶이 83%설명이 충분한지를 고려 할수 있음.
그리고 PC3 이후 설명 분산의 증가율이 감소하는거기때문에 3개가 적절한듯?

"""

# 2D PCA 시각화
plt.figure(figsize=(8, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k')
plt.title("PCA: First Two Principal Components")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.show()

from mpl_toolkits.mplot3d import Axes3D

# 3D PCA 시각화
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], c=kmeans.labels_, cmap='viridis', edgecolor='k')
ax.set_title("PCA: First Three Principal Components")
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.set_zlabel("Principal Component 3")
plt.show()

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# t-SNE 수행 (2D로 축소)
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
data_tsne = tsne.fit_transform(data_normalized)

# t-SNE 결과 확인
print(f"Shape of t-SNE result: {data_tsne.shape}")

plt.figure(figsize=(8, 6))
plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k', alpha=0.7)
plt.title("t-SNE Visualization (2D)")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.grid(True)
plt.colorbar(label="Cluster Label")
plt.show()

"""# T SNE 매개변수 최적화
- perplexity
- learning rate
- n_iter
"""

# 각 군집의 평균 값을 계산
# Select only numeric columns before calculating the mean
numeric_df = df.select_dtypes(include=np.number)
cluster_summary = numeric_df.groupby('Cluster').mean()
print(cluster_summary)

from sklearn.metrics import silhouette_score

# 실루엣 점수 계산
silhouette_avg = silhouette_score(data_normalized, kmeans.labels_)
print(f"Silhouette Score: {silhouette_avg}")

import seaborn as sns

# Cluster vs Spending Score 분포
sns.boxplot(x='Cluster', y='Spending Score (1-100)', data=df)
plt.title("Spending Score Distribution by Cluster")
plt.show()

# Cluster vs Age 분포
sns.boxplot(x='Cluster', y='Age', data=df)
plt.title("Age Distribution by Cluster")
plt.show()

from sklearn.cluster import DBSCAN

# DBSCAN 수행
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['DBSCAN_Cluster'] = dbscan.fit_predict(data_normalized)

# 결과 시각화
plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=df['DBSCAN_Cluster'], cmap='viridis', edgecolor='k')
plt.title("t-SNE with DBSCAN Clusters")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage

# 계층적 군집화 수행
linkage_matrix = linkage(data_normalized, method='ward')
dendrogram(linkage_matrix)
plt.title("Dendrogram")
plt.xlabel("Sample Index")
plt.ylabel("Distance")
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
data_normalized_df['Cluster'] = kmeans.fit_predict(data_normalized)

# Add cluster labels to the original dataset for visualization
df['Cluster'] = data_normalized_df['Cluster']

# Plot scatter plots for clusters
plt.figure(figsize=(15, 5))

# Scatter plot for Age vs Annual Income
plt.subplot(1, 2, 1)
for cluster in df['Cluster'].unique():
    cluster_data = df[df['Cluster'] == cluster]
    plt.scatter(cluster_data['Annual Income (k$)'], cluster_data['Age'], label=f"Cluster {cluster}")
plt.title('Clusters: Age vs Annual Income')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Age')
plt.legend()

# Scatter plot for Annual Income vs Spending Score
plt.subplot(1, 2, 2)
for cluster in df['Cluster'].unique():
    cluster_data = df[df['Cluster'] == cluster]
    plt.scatter(cluster_data['Annual Income (k$)'], cluster_data['Spending Score (1-100)'], label=f"Cluster {cluster}")
plt.title('Clusters: Annual Income vs Spending Score')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Initialize DBSCAN with a range of epsilon values to test
eps_values = [0.5, 1.0, 1.5, 2.0]
min_samples = 5  # Default minimum samples for DBSCAN

# Dictionary to store results for each epsilon value
dbscan_results = {}

# Use data_normalized_df instead of scaled_df
for eps in eps_values:
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels = dbscan.fit_predict(data_normalized_df)  # Changed here

    # Calculate the number of clusters (excluding noise, -1 label)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    # Silhouette score (only if there are at least 2 clusters)
    if n_clusters > 1:
        silhouette_avg = silhouette_score(data_normalized_df, labels)  # Changed here
    else:
        silhouette_avg = "N/A"

    dbscan_results[eps] = {"Clusters": n_clusters, "Silhouette Score": silhouette_avg}

# Convert results to DataFrame for clarity
dbscan_results_df = pd.DataFrame(dbscan_results).T
dbscan_results_df.index.name = "Epsilon (eps)"

display(dbscan_results_df)



# Perform DBSCAN with a chosen epsilon value (e.g., eps=0.5 for better visualization)

#Remove the extra space before dbscan
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(data_normalized_df)  # Changed here

# Add cluster labels to the DataFrame for visualization
data_normalized_df['Cluster'] = labels  # Changed here

# Scatter plot of clusters (using the first two dimensions for 2D visualization)
plt.figure(figsize=(8, 6))
plt.scatter(data_normalized_df['Age'], data_normalized_df['Annual Income (k$)'], c=labels, cmap='viridis', s=50)  # Changed here
plt.title('DBSCAN Clustering Visualization')
plt.xlabel('Age (scaled)')
plt.ylabel('Annual Income (scaled)')
plt.colorbar(label='Cluster Label')
plt.show()

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import numpy as np

# Perform hierarchical clustering using Ward's method
linkage_matrix = linkage(data_normalized_df.iloc[:, :-1], method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix, truncate_mode='lastp', p=10, leaf_rotation=45, leaf_font_size=12)
plt.title('Hierarchical Clustering Dendrogram (Truncated)')
plt.xlabel('Cluster Size')
plt.ylabel('Distance')
plt.show()

# Cut the dendrogram at a specific number of clusters (e.g., 3)
n_clusters = 3
hc_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')

# Add hierarchical cluster labels to the scaled DataFrame
data_normalized_df['HC_Cluster'] = hc_labels

# Show the cluster distribution
data_normalized_df['HC_Cluster'].value_counts()

# Scatter plot of hierarchical clustering (using the first two dimensions for 2D visualization)
plt.figure(figsize=(8, 6))
plt.scatter(
    data_normalized_df['Age'],
    data_normalized_df['Annual Income (k$)'],
    c=data_normalized_df['HC_Cluster'],
    cmap='tab10',
    s=50
)
plt.title('Hierarchical Clustering Visualization')
plt.xlabel('Age (scaled)')
plt.ylabel('Annual Income (scaled)')
plt.colorbar(label='Cluster Label')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Initialize an empty list to store the Within-Cluster Sum of Squares (WCSS)
wcss = []

# Test K-means clustering for cluster numbers 1 to 10
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    # Use data_normalized_df instead of scaled_df
    kmeans.fit(data_normalized_df)
    wcss.append(kmeans.inertia_)

# Plot the WCSS values to use the Elbow method
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Apply KMeans clustering
kmeans = KMeans(n_clusters=10, random_state=42)
data_normalized_df['Cluster'] = kmeans.fit_predict(data_normalized)

# Add cluster labels to the original dataset for visualization
df['Cluster'] = data_normalized_df['Cluster']

# Plot scatter plots for clusters
plt.figure(figsize=(15, 5))

# Scatter plot for Age vs Annual Income
plt.subplot(1, 2, 1)
for cluster in df['Cluster'].unique():
    cluster_data = df[df['Cluster'] == cluster]
    plt.scatter(cluster_data['Annual Income (k$)'], cluster_data['Age'], label=f"Cluster {cluster}")
plt.title('Clusters: Age vs Annual Income')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Age')
plt.legend()

# Scatter plot for Annual Income vs Spending Score
plt.subplot(1, 2, 2)
for cluster in df['Cluster'].unique():
    cluster_data = df[df['Cluster'] == cluster]
    plt.scatter(cluster_data['Annual Income (k$)'], cluster_data['Spending Score (1-100)'], label=f"Cluster {cluster}")
plt.title('Clusters: Annual Income vs Spending Score')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Initialize DBSCAN with a range of epsilon values to test
eps_values = [0.5, 1.0, 1.5, 2.0]
min_samples = 10  # Default minimum samples for DBSCAN

# Dictionary to store results for each epsilon value
dbscan_results = {}

# Use data_normalized_df instead of scaled_df
for eps in eps_values:
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels = dbscan.fit_predict(data_normalized_df)  # Changed here

    # Calculate the number of clusters (excluding noise, -1 label)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    # Silhouette score (only if there are at least 2 clusters)
    if n_clusters > 1:
        silhouette_avg = silhouette_score(data_normalized_df, labels)  # Changed here
    else:
        silhouette_avg = "N/A"

    dbscan_results[eps] = {"Clusters": n_clusters, "Silhouette Score": silhouette_avg}

# Convert results to DataFrame for clarity
dbscan_results_df = pd.DataFrame(dbscan_results).T
dbscan_results_df.index.name = "Epsilon (eps)"

display(dbscan_results_df)



# Perform DBSCAN with a chosen epsilon value (e.g., eps=0.5 for better visualization)

#Remove the extra space before dbscan
dbscan = DBSCAN(eps=0.5, min_samples=10)
labels = dbscan.fit_predict(data_normalized_df)  # Changed here

# Add cluster labels to the DataFrame for visualization
data_normalized_df['Cluster'] = labels  # Changed here

# Scatter plot of clusters (using the first two dimensions for 2D visualization)
plt.figure(figsize=(8, 6))
plt.scatter(data_normalized_df['Age'], data_normalized_df['Annual Income (k$)'], c=labels, cmap='viridis', s=50)  # Changed here
plt.title('DBSCAN Clustering Visualization')
plt.xlabel('Age (scaled)')
plt.ylabel('Annual Income (scaled)')
plt.colorbar(label='Cluster Label')
plt.show()

# Group data by clusters and calculate descriptive statistics for each cluster
cluster_analysis = df.groupby('Cluster').agg({
    'Age': ['mean', 'std', 'min', 'max'],
    'Annual Income (k$)': ['mean', 'std', 'min', 'max'],
    'Spending Score (1-100)': ['mean', 'std', 'min', 'max'],
    'Gender': lambda x: x.mode()[0]  # Mode for categorical feature
}).rename(columns={"<lambda_0>": "Most Common Gender"})

# Display the characteristics of each cluster
display(dataframe=cluster_analysis)
Anlaysis_by_feature = cluster_analysis.T
Anlaysis_by_feature

sns.pairplot(df)

for col in df.select_dtypes(include=['int', 'float']).columns:
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.set_title(f'Boxplot of {col}')
    bp = sns.boxplot(data=df, x=col, ax=ax)
    plt.show()
    plt.close(fig)

sns.displot(data=df, x="Gender")

sns.displot(df, x = 'Age',hue='Gender', kind='kde')
plt.title('Distribution Plot of the Age of customers')

sns.distplot(df['Age'], kde=True, bins=30)
plt.title('Distribution Plot of the Age of customers')
plt.show()

sns.distplot(df['Annual Income (k$)'], kde=True, bins=20)
plt.title('Distribution Plot of the Anual income of customers')
plt.show()

sns.set_palette("Set2")
sns.distplot(df["Spending Score (1-100)"], kde=True, bins=30)
plt.title('Distribution Plot of the Spending Score of customers')
plt.show()

custom_palette = "Set2"
sns.violinplot(x='Age', y='Gender', data=df, palette=custom_palette)
plt.title('Violinplot of gender with the age ')
plt.show()

custom_palette = "Set2"
sns.violinplot(x="Spending Score (1-100)", y='Gender', data=df, palette=custom_palette)
plt.title('Violinplot of gender and the spending scores')
plt.show()

custom_palette = "Set2"
sns.violinplot(x="Annual Income (k$)", y='Gender', data=df, palette=custom_palette)
plt.title('Violinplot of gender and the annual income')
plt.show()

custom_palette = "Set2"
sns.set_palette(custom_palette)
plt.figure(figsize=(8, 6))
df['Gender'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette()
                                   , startangle=140)
plt.title("Distribution of Gender")
plt.ylabel("")
plt.axis('equal')
plt.show()

custom_palette = "Set2"
sns.set_palette(custom_palette)
plt.figure(figsize=(20,10))
sns.boxplot(data=df, x="Age", y="Annual Income (k$)")
plt.show()

custom_palette = "Set2"
sns.set_palette(custom_palette)
plt.figure(figsize=(20,10))
sns.boxplot(data=df, x="Age", y="Spending Score (1-100)")
plt.show()